{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import keras\nfrom keras.layers import Dense, Activation, Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dropout\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\nfrom keras.models import load_model\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils.np_utils import to_categorical\nfrom keras.datasets import mnist, fashion_mnist\nfrom keras import optimizers\nimport pandas as pd\nimport itertools","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)\n\nprint(len(x_train))\nprint(len(y_train))\nprint(len(x_test))\nprint(len(y_test))\n\nplt.subplot(1,3,1)\nplt.imshow(x_train[0],cmap='gray')\nplt.title('Label'+ ' ' + str(y_train[0]))\nplt.subplot(1,3,2)\nplt.imshow(x_train[10000],cmap='gray')\nplt.title('Label'+ ' ' + str(y_train[10000]))\nplt.subplot(1,3,3)\nplt.imshow(x_test[6000],cmap='gray')\nplt.title('Label'+ ' ' + str(y_test[6000]))\n\nprint('\\n')\nprint(str(np.unique(y_train)) + ' \\n ' + 'Number of classes' + ' ' + str(len(np.unique(y_train))))\n\nlab = ['T-Shirt/Top','Trouser/Pants','Pullover Shirt','Dress',\n      'Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n\nlab_des = pd.DataFrame(np.unique(y_train), lab)\nlab_des.head(10)","execution_count":37,"outputs":[{"output_type":"stream","text":"(60000, 28, 28)\n(60000,)\n(10000, 28, 28)\n(10000,)\n60000\n60000\n10000\n10000\n\n\n[0 1 2 3 4 5 6 7 8 9] \n Number of classes 10\n","name":"stdout"},{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"                0\nT-Shirt/Top     0\nTrouser/Pants   1\nPullover Shirt  2\nDress           3\nCoat            4\nSandal          5\nShirt           6\nSneaker         7\nBag             8\nAnkle boot      9","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>T-Shirt/Top</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Trouser/Pants</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Pullover Shirt</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>Dress</th>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>Coat</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>Sandal</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>Shirt</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>Sneaker</th>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>Bag</th>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>Ankle boot</th>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 3 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.reshape(x_train.shape[0],28,28,1).astype('float32')\nx_test = x_test.reshape(x_test.shape[0],28,28,1).astype('float32')\nx_train = x_train / 255\nx_test = x_test / 255\n\nlaben = LabelEncoder()\ny_train = laben.fit_transform(y_train) # This gives Label (Eg 3)\ny_train = to_categorical(y_train,10) # Do One hot encoding (Eg [0,0,0,1,0....0])\ny_test = laben.fit_transform(y_test) \ny_test = to_categorical(y_test,10) \n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":38,"outputs":[{"output_type":"stream","text":"(60000, 28, 28, 1)\n(10000, 28, 28, 1)\n(60000, 10)\n(10000, 10)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(128, (3, 3), padding='same',activation='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\t\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.summary()","execution_count":39,"outputs":[{"output_type":"stream","text":"Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_13 (Conv2D)           (None, 28, 28, 128)       1280      \n_________________________________________________________________\nconv2d_14 (Conv2D)           (None, 26, 26, 64)        73792     \n_________________________________________________________________\nmax_pooling2d_9 (MaxPooling2 (None, 13, 13, 64)        0         \n_________________________________________________________________\ndropout_13 (Dropout)         (None, 13, 13, 64)        0         \n_________________________________________________________________\nconv2d_15 (Conv2D)           (None, 13, 13, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_10 (MaxPooling (None, 6, 6, 64)          0         \n_________________________________________________________________\ndropout_14 (Dropout)         (None, 6, 6, 64)          0         \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 2304)              0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 512)               1180160   \n_________________________________________________________________\ndropout_15 (Dropout)         (None, 512)               0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 10)                5130      \n=================================================================\nTotal params: 1,297,290\nTrainable params: 1,297,290\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint('/kaggle/working/model.h5',\n                             monitor = 'val_loss',\n                             mode = 'min',\n                             save_best_only=True,\n                             verbose = 1\n                            )\n\ncallbacks = [checkpoint]","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x_train, y_train,\n                   epochs = 100,\n                   validation_data=(x_test,y_test),\n                   batch_size=16,\n                  callbacks = callbacks\n                   )\nmodel.save('/kaggle/working/model_1.h5')","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch 1/100\n3750/3750 [==============================] - ETA: 0s - loss: 0.6380 - accuracy: 0.7653\nEpoch 00001: val_loss improved from inf to 0.41848, saving model to /kaggle/working/model.h5\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.6380 - accuracy: 0.7653 - val_loss: 0.4185 - val_accuracy: 0.8491\nEpoch 2/100\n3745/3750 [============================>.] - ETA: 0s - loss: 0.3976 - accuracy: 0.8571\nEpoch 00002: val_loss improved from 0.41848 to 0.34571, saving model to /kaggle/working/model.h5\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3975 - accuracy: 0.8572 - val_loss: 0.3457 - val_accuracy: 0.8742\nEpoch 3/100\n3746/3750 [============================>.] - ETA: 0s - loss: 0.3394 - accuracy: 0.8784\nEpoch 00003: val_loss improved from 0.34571 to 0.30670, saving model to /kaggle/working/model.h5\n3750/3750 [==============================] - 18s 5ms/step - loss: 0.3393 - accuracy: 0.8783 - val_loss: 0.3067 - val_accuracy: 0.8913\nEpoch 4/100\n3750/3750 [==============================] - ETA: 0s - loss: 0.3090 - accuracy: 0.8902\nEpoch 00004: val_loss improved from 0.30670 to 0.28592, saving model to /kaggle/working/model.h5\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.3090 - accuracy: 0.8902 - val_loss: 0.2859 - val_accuracy: 0.9005\nEpoch 5/100\n3744/3750 [============================>.] - ETA: 0s - loss: 0.2893 - accuracy: 0.8969\nEpoch 00005: val_loss did not improve from 0.28592\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.2895 - accuracy: 0.8969 - val_loss: 0.2935 - val_accuracy: 0.9013\nEpoch 6/100\n3748/3750 [============================>.] - ETA: 0s - loss: 0.2787 - accuracy: 0.9006\nEpoch 00006: val_loss improved from 0.28592 to 0.27104, saving model to /kaggle/working/model.h5\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.2789 - accuracy: 0.9006 - val_loss: 0.2710 - val_accuracy: 0.9056\nEpoch 7/100\n3746/3750 [============================>.] - ETA: 0s - loss: 0.2676 - accuracy: 0.9067\nEpoch 00007: val_loss did not improve from 0.27104\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.2678 - accuracy: 0.9067 - val_loss: 0.3003 - val_accuracy: 0.9093\nEpoch 8/100\n3743/3750 [============================>.] - ETA: 0s - loss: 0.2637 - accuracy: 0.9071\nEpoch 00008: val_loss improved from 0.27104 to 0.25820, saving model to /kaggle/working/model.h5\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.2636 - accuracy: 0.9071 - val_loss: 0.2582 - val_accuracy: 0.9098\nEpoch 9/100\n3742/3750 [============================>.] - ETA: 0s - loss: 0.2603 - accuracy: 0.9094\nEpoch 00009: val_loss did not improve from 0.25820\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.2603 - accuracy: 0.9093 - val_loss: 0.2783 - val_accuracy: 0.9046\nEpoch 10/100\n3749/3750 [============================>.] - ETA: 0s - loss: 0.2586 - accuracy: 0.9099\nEpoch 00010: val_loss did not improve from 0.25820\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.2586 - accuracy: 0.9099 - val_loss: 0.2655 - val_accuracy: 0.9112\nEpoch 11/100\n3738/3750 [============================>.] - ETA: 0s - loss: 0.2571 - accuracy: 0.9099\nEpoch 00011: val_loss did not improve from 0.25820\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.2573 - accuracy: 0.9099 - val_loss: 0.2701 - val_accuracy: 0.9084\nEpoch 12/100\n3739/3750 [============================>.] - ETA: 0s - loss: 0.2542 - accuracy: 0.9116\nEpoch 00012: val_loss did not improve from 0.25820\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.2542 - accuracy: 0.9117 - val_loss: 0.2636 - val_accuracy: 0.9171\nEpoch 13/100\n3748/3750 [============================>.] - ETA: 0s - loss: 0.2530 - accuracy: 0.9114\nEpoch 00013: val_loss improved from 0.25820 to 0.25631, saving model to /kaggle/working/model.h5\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.2529 - accuracy: 0.9115 - val_loss: 0.2563 - val_accuracy: 0.9083\nEpoch 14/100\n3749/3750 [============================>.] - ETA: 0s - loss: 0.2522 - accuracy: 0.9138\nEpoch 00014: val_loss improved from 0.25631 to 0.24845, saving model to /kaggle/working/model.h5\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.2522 - accuracy: 0.9138 - val_loss: 0.2484 - val_accuracy: 0.9129\nEpoch 15/100\n3738/3750 [============================>.] - ETA: 0s - loss: 0.2524 - accuracy: 0.9129\nEpoch 00015: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.2522 - accuracy: 0.9130 - val_loss: 0.2625 - val_accuracy: 0.9086\nEpoch 16/100\n3741/3750 [============================>.] - ETA: 0s - loss: 0.2535 - accuracy: 0.9136\nEpoch 00016: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.2535 - accuracy: 0.9136 - val_loss: 0.2683 - val_accuracy: 0.9130\nEpoch 17/100\n3749/3750 [============================>.] - ETA: 0s - loss: 0.2550 - accuracy: 0.9133\nEpoch 00017: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.2550 - accuracy: 0.9133 - val_loss: 0.2951 - val_accuracy: 0.9079\nEpoch 18/100\n3739/3750 [============================>.] - ETA: 0s - loss: 0.2548 - accuracy: 0.9128\nEpoch 00018: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.2548 - accuracy: 0.9129 - val_loss: 0.3107 - val_accuracy: 0.9050\nEpoch 19/100\n3750/3750 [==============================] - ETA: 0s - loss: 0.2545 - accuracy: 0.9128\nEpoch 00019: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.2545 - accuracy: 0.9128 - val_loss: 0.2867 - val_accuracy: 0.9115\nEpoch 20/100\n3743/3750 [============================>.] - ETA: 0s - loss: 0.2619 - accuracy: 0.9113\nEpoch 00020: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.2619 - accuracy: 0.9113 - val_loss: 0.3170 - val_accuracy: 0.9066\nEpoch 21/100\n3741/3750 [============================>.] - ETA: 0s - loss: 0.2614 - accuracy: 0.9125\nEpoch 00021: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.2615 - accuracy: 0.9125 - val_loss: 0.2843 - val_accuracy: 0.9120\nEpoch 22/100\n3740/3750 [============================>.] - ETA: 0s - loss: 0.2646 - accuracy: 0.9092\nEpoch 00022: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.2650 - accuracy: 0.9090 - val_loss: 0.3425 - val_accuracy: 0.9067\nEpoch 23/100\n3743/3750 [============================>.] - ETA: 0s - loss: 0.2677 - accuracy: 0.9096\nEpoch 00023: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.2676 - accuracy: 0.9096 - val_loss: 0.2870 - val_accuracy: 0.9044\nEpoch 24/100\n3750/3750 [==============================] - ETA: 0s - loss: 0.2722 - accuracy: 0.9087\nEpoch 00024: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.2722 - accuracy: 0.9087 - val_loss: 0.3299 - val_accuracy: 0.8993\nEpoch 25/100\n3740/3750 [============================>.] - ETA: 0s - loss: 0.2757 - accuracy: 0.9074\nEpoch 00025: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.2756 - accuracy: 0.9075 - val_loss: 0.2909 - val_accuracy: 0.9032\nEpoch 26/100\n3748/3750 [============================>.] - ETA: 0s - loss: 0.2778 - accuracy: 0.9052\nEpoch 00026: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.2778 - accuracy: 0.9052 - val_loss: 0.2859 - val_accuracy: 0.9109\nEpoch 27/100\n3741/3750 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9041\nEpoch 00027: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.2852 - accuracy: 0.9039 - val_loss: 0.3351 - val_accuracy: 0.8971\nEpoch 28/100\n3750/3750 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.9039\nEpoch 00028: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.2864 - accuracy: 0.9039 - val_loss: 0.3721 - val_accuracy: 0.8985\n","name":"stdout"},{"output_type":"stream","text":"Epoch 29/100\n3743/3750 [============================>.] - ETA: 0s - loss: 0.2920 - accuracy: 0.9024\nEpoch 00029: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.2919 - accuracy: 0.9025 - val_loss: 0.2751 - val_accuracy: 0.9103\nEpoch 30/100\n3742/3750 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.9005\nEpoch 00030: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.2977 - accuracy: 0.9005 - val_loss: 0.3399 - val_accuracy: 0.9085\nEpoch 31/100\n3749/3750 [============================>.] - ETA: 0s - loss: 0.2971 - accuracy: 0.8999\nEpoch 00031: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.2970 - accuracy: 0.8999 - val_loss: 0.2918 - val_accuracy: 0.9019\nEpoch 32/100\n3746/3750 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.9001\nEpoch 00032: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.2995 - accuracy: 0.9002 - val_loss: 0.3502 - val_accuracy: 0.9042\nEpoch 33/100\n3744/3750 [============================>.] - ETA: 0s - loss: 0.3014 - accuracy: 0.8996\nEpoch 00033: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.3015 - accuracy: 0.8996 - val_loss: 0.3964 - val_accuracy: 0.9012\nEpoch 34/100\n3743/3750 [============================>.] - ETA: 0s - loss: 0.3027 - accuracy: 0.8989\nEpoch 00034: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.3027 - accuracy: 0.8989 - val_loss: 0.2729 - val_accuracy: 0.9079\nEpoch 35/100\n3742/3750 [============================>.] - ETA: 0s - loss: 0.3019 - accuracy: 0.8978\nEpoch 00035: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3018 - accuracy: 0.8978 - val_loss: 0.2758 - val_accuracy: 0.9037\nEpoch 36/100\n3741/3750 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.8981\nEpoch 00036: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.3045 - accuracy: 0.8982 - val_loss: 0.3205 - val_accuracy: 0.9032\nEpoch 37/100\n3748/3750 [============================>.] - ETA: 0s - loss: 0.3094 - accuracy: 0.8967\nEpoch 00037: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.3094 - accuracy: 0.8967 - val_loss: 0.2941 - val_accuracy: 0.8996\nEpoch 38/100\n3746/3750 [============================>.] - ETA: 0s - loss: 0.3075 - accuracy: 0.8971\nEpoch 00038: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.3074 - accuracy: 0.8971 - val_loss: 0.3510 - val_accuracy: 0.8996\nEpoch 39/100\n3745/3750 [============================>.] - ETA: 0s - loss: 0.3120 - accuracy: 0.8952\nEpoch 00039: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3121 - accuracy: 0.8952 - val_loss: 0.3691 - val_accuracy: 0.8931\nEpoch 40/100\n3745/3750 [============================>.] - ETA: 0s - loss: 0.3168 - accuracy: 0.8946\nEpoch 00040: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.3168 - accuracy: 0.8946 - val_loss: 0.3077 - val_accuracy: 0.8987\nEpoch 41/100\n3747/3750 [============================>.] - ETA: 0s - loss: 0.3179 - accuracy: 0.8945\nEpoch 00041: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.3178 - accuracy: 0.8945 - val_loss: 0.3022 - val_accuracy: 0.8978\nEpoch 42/100\n3747/3750 [============================>.] - ETA: 0s - loss: 0.3169 - accuracy: 0.8942\nEpoch 00042: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3170 - accuracy: 0.8942 - val_loss: 0.3916 - val_accuracy: 0.8904\nEpoch 43/100\n3744/3750 [============================>.] - ETA: 0s - loss: 0.3193 - accuracy: 0.8942\nEpoch 00043: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.3191 - accuracy: 0.8942 - val_loss: 0.3134 - val_accuracy: 0.9037\nEpoch 44/100\n3748/3750 [============================>.] - ETA: 0s - loss: 0.3235 - accuracy: 0.8916\nEpoch 00044: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.3234 - accuracy: 0.8916 - val_loss: 0.3227 - val_accuracy: 0.8966\nEpoch 45/100\n3744/3750 [============================>.] - ETA: 0s - loss: 0.3235 - accuracy: 0.8944\nEpoch 00045: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.3235 - accuracy: 0.8943 - val_loss: 0.3175 - val_accuracy: 0.8966\nEpoch 46/100\n3743/3750 [============================>.] - ETA: 0s - loss: 0.3247 - accuracy: 0.8931\nEpoch 00046: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3248 - accuracy: 0.8931 - val_loss: 0.3309 - val_accuracy: 0.8951\nEpoch 47/100\n3746/3750 [============================>.] - ETA: 0s - loss: 0.3244 - accuracy: 0.8912\nEpoch 00047: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.3242 - accuracy: 0.8912 - val_loss: 0.3211 - val_accuracy: 0.8983\nEpoch 48/100\n3748/3750 [============================>.] - ETA: 0s - loss: 0.3285 - accuracy: 0.8910\nEpoch 00048: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.3285 - accuracy: 0.8910 - val_loss: 0.3473 - val_accuracy: 0.8937\nEpoch 49/100\n3738/3750 [============================>.] - ETA: 0s - loss: 0.3252 - accuracy: 0.8922\nEpoch 00049: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.3250 - accuracy: 0.8922 - val_loss: 0.2838 - val_accuracy: 0.9036\nEpoch 50/100\n3740/3750 [============================>.] - ETA: 0s - loss: 0.3325 - accuracy: 0.8908\nEpoch 00050: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3324 - accuracy: 0.8908 - val_loss: 0.3872 - val_accuracy: 0.8935\nEpoch 51/100\n3745/3750 [============================>.] - ETA: 0s - loss: 0.3284 - accuracy: 0.8914\nEpoch 00051: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.3287 - accuracy: 0.8913 - val_loss: 0.3826 - val_accuracy: 0.8981\nEpoch 52/100\n3744/3750 [============================>.] - ETA: 0s - loss: 0.3311 - accuracy: 0.8901\nEpoch 00052: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.3311 - accuracy: 0.8901 - val_loss: 0.3236 - val_accuracy: 0.9003\nEpoch 53/100\n3741/3750 [============================>.] - ETA: 0s - loss: 0.3315 - accuracy: 0.8899\nEpoch 00053: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3317 - accuracy: 0.8898 - val_loss: 0.3235 - val_accuracy: 0.8984\nEpoch 54/100\n3746/3750 [============================>.] - ETA: 0s - loss: 0.3363 - accuracy: 0.8904\nEpoch 00054: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.3363 - accuracy: 0.8904 - val_loss: 0.3094 - val_accuracy: 0.8986\nEpoch 55/100\n3749/3750 [============================>.] - ETA: 0s - loss: 0.3314 - accuracy: 0.8902\nEpoch 00055: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.3315 - accuracy: 0.8902 - val_loss: 0.3571 - val_accuracy: 0.8980\nEpoch 56/100\n3750/3750 [==============================] - ETA: 0s - loss: 0.3370 - accuracy: 0.8890\nEpoch 00056: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 16s 4ms/step - loss: 0.3370 - accuracy: 0.8890 - val_loss: 0.2978 - val_accuracy: 0.9020\nEpoch 57/100\n3742/3750 [============================>.] - ETA: 0s - loss: 0.3323 - accuracy: 0.8881\nEpoch 00057: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 18s 5ms/step - loss: 0.3323 - accuracy: 0.8880 - val_loss: 0.3058 - val_accuracy: 0.8976\n","name":"stdout"},{"output_type":"stream","text":"Epoch 58/100\n3750/3750 [==============================] - ETA: 0s - loss: 0.3365 - accuracy: 0.8881\nEpoch 00058: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.3365 - accuracy: 0.8881 - val_loss: 0.3821 - val_accuracy: 0.8951\nEpoch 59/100\n3741/3750 [============================>.] - ETA: 0s - loss: 0.3372 - accuracy: 0.8880\nEpoch 00059: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3373 - accuracy: 0.8881 - val_loss: 0.3485 - val_accuracy: 0.8919\nEpoch 60/100\n3745/3750 [============================>.] - ETA: 0s - loss: 0.3354 - accuracy: 0.8900\nEpoch 00060: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 18s 5ms/step - loss: 0.3352 - accuracy: 0.8901 - val_loss: 0.3582 - val_accuracy: 0.8924\nEpoch 61/100\n3742/3750 [============================>.] - ETA: 0s - loss: 0.3377 - accuracy: 0.8875\nEpoch 00061: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3378 - accuracy: 0.8875 - val_loss: 0.4261 - val_accuracy: 0.8957\nEpoch 62/100\n3750/3750 [==============================] - ETA: 0s - loss: 0.3328 - accuracy: 0.8873\nEpoch 00062: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.3328 - accuracy: 0.8873 - val_loss: 0.3329 - val_accuracy: 0.8947\nEpoch 63/100\n3742/3750 [============================>.] - ETA: 0s - loss: 0.3349 - accuracy: 0.8883\nEpoch 00063: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3351 - accuracy: 0.8882 - val_loss: 0.3308 - val_accuracy: 0.8964\nEpoch 64/100\n3739/3750 [============================>.] - ETA: 0s - loss: 0.3333 - accuracy: 0.8886\nEpoch 00064: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3333 - accuracy: 0.8886 - val_loss: 0.3072 - val_accuracy: 0.9042\nEpoch 65/100\n3738/3750 [============================>.] - ETA: 0s - loss: 0.3332 - accuracy: 0.8887\nEpoch 00065: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3329 - accuracy: 0.8888 - val_loss: 0.3315 - val_accuracy: 0.8963\nEpoch 66/100\n3747/3750 [============================>.] - ETA: 0s - loss: 0.3311 - accuracy: 0.8882\nEpoch 00066: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 4ms/step - loss: 0.3311 - accuracy: 0.8882 - val_loss: 0.3202 - val_accuracy: 0.8987\nEpoch 67/100\n3738/3750 [============================>.] - ETA: 0s - loss: 0.3364 - accuracy: 0.8896\nEpoch 00067: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 18s 5ms/step - loss: 0.3363 - accuracy: 0.8897 - val_loss: 0.5274 - val_accuracy: 0.8924\nEpoch 68/100\n3748/3750 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.8892\nEpoch 00068: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3368 - accuracy: 0.8892 - val_loss: 0.3132 - val_accuracy: 0.8998\nEpoch 69/100\n3750/3750 [==============================] - ETA: 0s - loss: 0.3332 - accuracy: 0.8878\nEpoch 00069: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3332 - accuracy: 0.8878 - val_loss: 0.2959 - val_accuracy: 0.9004\nEpoch 70/100\n3744/3750 [============================>.] - ETA: 0s - loss: 0.3318 - accuracy: 0.8896\nEpoch 00070: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3316 - accuracy: 0.8897 - val_loss: 0.3687 - val_accuracy: 0.8908\nEpoch 71/100\n3745/3750 [============================>.] - ETA: 0s - loss: 0.3337 - accuracy: 0.8894\nEpoch 00071: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3339 - accuracy: 0.8893 - val_loss: 0.3114 - val_accuracy: 0.8986\nEpoch 72/100\n3742/3750 [============================>.] - ETA: 0s - loss: 0.3332 - accuracy: 0.8877\nEpoch 00072: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3330 - accuracy: 0.8878 - val_loss: 0.3202 - val_accuracy: 0.9011\nEpoch 73/100\n3741/3750 [============================>.] - ETA: 0s - loss: 0.3293 - accuracy: 0.8897\nEpoch 00073: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3292 - accuracy: 0.8898 - val_loss: 0.4152 - val_accuracy: 0.8942\nEpoch 74/100\n3741/3750 [============================>.] - ETA: 0s - loss: 0.3340 - accuracy: 0.8894\nEpoch 00074: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 18s 5ms/step - loss: 0.3339 - accuracy: 0.8894 - val_loss: 0.3325 - val_accuracy: 0.8960\nEpoch 75/100\n3748/3750 [============================>.] - ETA: 0s - loss: 0.3302 - accuracy: 0.8893\nEpoch 00075: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 18s 5ms/step - loss: 0.3302 - accuracy: 0.8892 - val_loss: 0.2998 - val_accuracy: 0.8976\nEpoch 76/100\n3740/3750 [============================>.] - ETA: 0s - loss: 0.3274 - accuracy: 0.8905\nEpoch 00076: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3274 - accuracy: 0.8906 - val_loss: 0.3141 - val_accuracy: 0.8933\nEpoch 77/100\n3740/3750 [============================>.] - ETA: 0s - loss: 0.3368 - accuracy: 0.8883\nEpoch 00077: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 18s 5ms/step - loss: 0.3367 - accuracy: 0.8883 - val_loss: 0.2950 - val_accuracy: 0.9005\nEpoch 78/100\n3740/3750 [============================>.] - ETA: 0s - loss: 0.3343 - accuracy: 0.8883\nEpoch 00078: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3344 - accuracy: 0.8882 - val_loss: 0.3203 - val_accuracy: 0.8973\nEpoch 79/100\n3743/3750 [============================>.] - ETA: 0s - loss: 0.3324 - accuracy: 0.8885\nEpoch 00079: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3324 - accuracy: 0.8884 - val_loss: 0.4707 - val_accuracy: 0.8853\nEpoch 80/100\n3750/3750 [==============================] - ETA: 0s - loss: 0.3398 - accuracy: 0.8875\nEpoch 00080: val_loss did not improve from 0.24845\n3750/3750 [==============================] - 17s 5ms/step - loss: 0.3398 - accuracy: 0.8875 - val_loss: 0.4455 - val_accuracy: 0.8877\nEpoch 81/100\n  27/3750 [..............................] - ETA: 14s - loss: 0.3656 - accuracy: 0.8796","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"Score = model.evaluate(x_test,y_test)\nprint(' Test Accuracy ', Score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_ = np.argmax(y_test, axis=1)\ny_pred_ = model.predict_classes(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Classification Report \\n')\nprint(classification_report(y_test_, y_pred_, target_names = lab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\n\nclasses = 10\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n\nconfusion_mtx = confusion_matrix(y_test_, y_pred_) \nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}